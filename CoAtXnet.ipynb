{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhokxbSlpVIyhRO8SRoEM6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Husseinhhameed/CoAtXnet/blob/main/CoAtXnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fire"
      ],
      "metadata": {
        "id": "R-Rss2_HkOe8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkmL8zH3wyHJ",
        "outputId": "a1b1e5ee-afce-498b-dfb4-2d67ddeb2b1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transforms3d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEEza7plxLIl",
        "outputId": "f5a1a5fe-08a3-4c62-99b6-8662b74fad59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transforms3d in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from transforms3d) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install einops\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtc3-wAZIDSx",
        "outputId": "913ba0ad-4190-4332-93f0-679d9707acaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomRotation, ToTensor\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "from torchvision.utils import make_grid\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "from sklearn.model_selection import KFold\n",
        "from tqdm import tqdm\n",
        "import transforms3d.quaternions as txq\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "hZ_K431TID_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ModifiedDSACLoss(y_true, y_pred, weight_translation_initial=1.0, weight_quaternion_initial=1.0):\n",
        "    # Split the true and predicted values into delta positions and quaternions\n",
        "    true_positions, true_quaternions = torch.split(y_true, [3, 4], dim=-1)\n",
        "    pred_positions, pred_quaternions = torch.split(y_pred, [3, 4], dim=-1)\n",
        "\n",
        "    # Normalize the predicted quaternions\n",
        "    pred_quaternions = F.normalize(pred_quaternions, p=2, dim=-1)\n",
        "\n",
        "    # Calculate the translation error (L2 distance)\n",
        "    t_error = torch.sqrt(torch.sum((true_positions - pred_positions) ** 2, dim=-1))\n",
        "    mean_t_error = torch.mean(t_error)\n",
        "\n",
        "    # Calculate the quaternion angle error\n",
        "    dot_product = torch.sum(true_quaternions * pred_quaternions, dim=-1)\n",
        "    dot_product = torch.clamp(dot_product, -1.0, 1.0)  # Ensure dot product is within [-1, 1] to avoid NaNs in acos\n",
        "    angle_error = 2.0 * torch.acos(torch.abs(dot_product))\n",
        "    mean_q_error = torch.mean(angle_error)\n",
        "\n",
        "    # Dynamic weight adjustment based on the error ratio\n",
        "    error_ratio = mean_t_error / (mean_q_error + 1e-8)  # Adding a small epsilon to avoid division by zero\n",
        "    weight_translation = weight_translation_initial * error_ratio\n",
        "    weight_quaternion = weight_quaternion_initial / error_ratio\n",
        "\n",
        "    # Apply dynamic weights to the respective errors\n",
        "    weighted_t_error = weight_translation * mean_t_error\n",
        "    weighted_q_error = weight_quaternion * mean_q_error\n",
        "\n",
        "    # Final combined weighted error\n",
        "    combined_weighted_error = weighted_t_error + weighted_q_error\n",
        "\n",
        "    return combined_weighted_error"
      ],
      "metadata": {
        "id": "4KYX1xIfIPPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def qlog(q):\n",
        "    q = q / np.linalg.norm(q)\n",
        "    sinhalftheta = np.linalg.norm(q[1:])\n",
        "    coshalftheta = q[0]\n",
        "    r = np.arctan2(sinhalftheta, coshalftheta)\n",
        "    if sinhalftheta > 1e-6:\n",
        "        qlog = r * q[1:] / sinhalftheta\n",
        "    else:\n",
        "        qlog = np.zeros(3)\n",
        "    return qlog\n",
        "\n",
        "def process_poses(poses_in, mean_t, std_t, align_R, align_t, align_s):\n",
        "    poses_out = np.zeros((len(poses_in), 6))\n",
        "    poses_out[:, 0:3] = poses_in[:, [3, 7, 11]]  # Translation components\n",
        "\n",
        "    # Align and process rotation\n",
        "    for i in range(len(poses_out)):\n",
        "        R = poses_in[i].reshape((3, 4))[:3, :3]\n",
        "        q = txq.mat2quat(np.dot(align_R, R))\n",
        "        q *= np.sign(q[0])  # Constrain to hemisphere\n",
        "        q = qlog(q)\n",
        "        poses_out[i, 3:] = q\n",
        "        t = poses_out[i, :3] - align_t\n",
        "        poses_out[i, :3] = align_s * np.dot(align_R, t[:, np.newaxis]).squeeze()\n",
        "\n",
        "    # Normalize translation\n",
        "    poses_out[:, :3] -= mean_t\n",
        "    poses_out[:, :3] /= std_t\n",
        "\n",
        "    return poses_out\n",
        "\n",
        "def load_and_process_poses(data_dir, seqs, train=True, real=False, vo_lib='orbslam'):\n",
        "    ps = {}\n",
        "    vo_stats = {}\n",
        "    all_poses = []\n",
        "    for seq in seqs:\n",
        "        seq_dir = os.path.join(data_dir, seq)\n",
        "        p_filenames = [n for n in os.listdir(seq_dir) if n.find('pose') >= 0]\n",
        "        if real:\n",
        "            pose_file = os.path.join(data_dir, '{:s}_poses'.format(vo_lib), seq)\n",
        "            pss = np.loadtxt(pose_file)\n",
        "            frame_idx = pss[:, 0].astype(int)\n",
        "            if vo_lib == 'libviso2':\n",
        "                frame_idx -= 1\n",
        "            ps[seq] = pss[:, 1:13]\n",
        "            vo_stats_filename = os.path.join(seq_dir, '{:s}_vo_stats.pkl'.format(vo_lib))\n",
        "            with open(vo_stats_filename, 'rb') as f:\n",
        "                vo_stats[seq] = pickle.load(f)\n",
        "        else:\n",
        "            frame_idx = np.array(range(len(p_filenames)), dtype=int)\n",
        "            pss = [np.loadtxt(os.path.join(seq_dir, 'frame-{:06d}.pose.txt'.format(i))).flatten()[:12] for i in frame_idx if os.path.exists(os.path.join(seq_dir, 'frame-{:06d}.pose.txt'.format(i)))]\n",
        "            ps[seq] = np.asarray(pss)\n",
        "            vo_stats[seq] = {'R': np.eye(3), 't': np.zeros(3), 's': 1}\n",
        "\n",
        "        all_poses.append(ps[seq])\n",
        "\n",
        "    all_poses = np.vstack(all_poses)\n",
        "    pose_stats_filename = os.path.join(data_dir, 'pose_stats.txt')\n",
        "    if train and not real:\n",
        "        mean_t = np.mean(all_poses[:, [3, 7, 11]], axis=0)\n",
        "        std_t = np.std(all_poses[:, [3, 7, 11]], axis=0)\n",
        "        np.savetxt(pose_stats_filename, np.vstack((mean_t, std_t)), fmt='%8.7f')\n",
        "    else:\n",
        "        mean_t, std_t = np.loadtxt(pose_stats_filename)\n",
        "\n",
        "    # Process and normalize poses\n",
        "    processed_poses = []\n",
        "    for seq in seqs:\n",
        "        pss = process_poses(poses_in=ps[seq], mean_t=mean_t, std_t=std_t,\n",
        "                            align_R=vo_stats[seq]['R'], align_t=vo_stats[seq]['t'],\n",
        "                            align_s=vo_stats[seq]['s'])\n",
        "        processed_poses.append(pss)\n",
        "\n",
        "    return np.vstack(processed_poses)\n",
        "\n",
        "# Define the transformation for the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Dataset class\n",
        "class FireDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.seqs = [seq for seq in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, seq))]\n",
        "        self.samples = self._load_samples()\n",
        "        self.processed_poses = self._load_processed_poses()\n",
        "\n",
        "        # Debugging prints\n",
        "        print(f\"Number of samples: {len(self.samples)}\")\n",
        "        print(f\"Number of processed poses: {self.processed_poses.shape[0]}\")\n",
        "\n",
        "        # Ensure consistency between samples and processed poses\n",
        "        min_length = min(len(self.samples), self.processed_poses.shape[0])\n",
        "        self.samples = self.samples[:min_length]\n",
        "        self.processed_poses = self.processed_poses[:min_length]\n",
        "\n",
        "    def _load_samples(self):\n",
        "        samples = []\n",
        "        for seq_folder in self.seqs:\n",
        "            seq_path = os.path.join(self.root_dir, seq_folder)\n",
        "            color_files = sorted([f for f in os.listdir(seq_path) if f.endswith('.color.png')])\n",
        "            depth_files = sorted([f for f in os.listdir(seq_path) if f.endswith('.depth.png')])\n",
        "            pose_files = sorted([f for f in os.listdir(seq_path) if f.endswith('.pose.txt')])\n",
        "\n",
        "            for color_file, depth_file, pose_file in zip(color_files, depth_files, pose_files):\n",
        "                samples.append((os.path.join(seq_path, color_file),\n",
        "                                os.path.join(seq_path, depth_file),\n",
        "                                os.path.join(seq_path, pose_file)))\n",
        "        return samples\n",
        "\n",
        "    def _load_processed_poses(self):\n",
        "        return load_and_process_poses(self.root_dir, self.seqs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self.processed_poses):\n",
        "            raise IndexError(f\"Index {idx} out of bounds for processed poses of size {len(self.processed_poses)}\")\n",
        "\n",
        "        color_path, depth_path, pose_path = self.samples[idx]\n",
        "\n",
        "        color_image = cv2.imread(color_path, cv2.IMREAD_COLOR)\n",
        "        depth_image = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
        "        pose_matrix = self.processed_poses[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            color_image = self.transform(color_image)\n",
        "            depth_image = (depth_image / depth_image.max() * 255).astype(np.uint8)\n",
        "            depth_image = self.transform(depth_image)\n",
        "\n",
        "        return color_image, depth_image, pose_matrix\n"
      ],
      "metadata": {
        "id": "KJT5_rMxw57t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_3x3_bn(inp, oup, image_size, downsample=False):\n",
        "    stride = 1 if downsample == False else 2\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.GELU()\n",
        "    )\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn, norm):\n",
        "        super().__init__()\n",
        "        self.norm = norm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class SE(nn.Module):\n",
        "    def __init__(self, inp, oup, expansion=0.25):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(oup, int(inp * expansion), bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(inp * expansion), oup, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class XMBConv(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, downsample=False, expansion=4):\n",
        "        super().__init__()\n",
        "        self.downsample = downsample\n",
        "        stride = 1 if self.downsample == False else 2\n",
        "        hidden_dim = int(inp * expansion)\n",
        "\n",
        "        if self.downsample:\n",
        "            self.pool = nn.MaxPool2d(3, 2, 1)\n",
        "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
        "            self.poolx = nn.MaxPool2d(3, 2, 1)\n",
        "            self.projx = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
        "\n",
        "        if expansion == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n",
        "                          1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "            self.convx = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n",
        "                          1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                # down-sample in the first conv\n",
        "                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n",
        "                          groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                SE(inp, hidden_dim),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "            self.convx = nn.Sequential(\n",
        "                # pw\n",
        "                # down-sample in the first conv\n",
        "                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n",
        "                          groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                SE(inp, hidden_dim),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n",
        "        self.convx = PreNorm(inp, self.convx, nn.BatchNorm2d)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x[1]\n",
        "        x = x[0]\n",
        "\n",
        "        if self.downsample:\n",
        "            out1 = self.proj(self.pool(x)) + self.conv(x)\n",
        "            out2 = self.projx(self.poolx(y)) + self.convx(y)\n",
        "        else:\n",
        "            out1 = x + self.conv(x)\n",
        "            out2 = y + self.convx(y)\n",
        "        return [out1, out2]\n",
        "\n",
        "class XAttention(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        project_out = not (heads == 1 and dim_head == inp)\n",
        "\n",
        "        self.ih, self.iw = image_size\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        # parameter table of relative position bias\n",
        "        self.relative_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
        "\n",
        "        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
        "        coords = torch.flatten(torch.stack(coords), 1)\n",
        "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
        "\n",
        "        relative_coords[0] += self.ih - 1\n",
        "        relative_coords[1] += self.iw - 1\n",
        "        relative_coords[0] *= 2 * self.iw - 1\n",
        "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
        "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
        "        self.register_buffer(\"relative_index\", relative_index)\n",
        "\n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
        "        self.to_kx = nn.Linear(inp, inner_dim * 1, bias=False)\n",
        "        self.to_qx = nn.Linear(inp, inner_dim * 1, bias=False)\n",
        "\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, oup),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        kx = self.to_kx(y)\n",
        "        qx = self.to_qx(x)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(\n",
        "            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
        "        kx = rearrange(kx, 'b n (h d) -> b h n d', h=self.heads)\n",
        "        qx = rearrange(qx, 'b n (h d) -> b h n d', h=self.heads)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "        # Use \"gather\" for more efficiency on GPUs\n",
        "        relative_bias = self.relative_bias_table.gather(\n",
        "            0, self.relative_index.repeat(1, self.heads))\n",
        "        relative_bias = rearrange(\n",
        "            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
        "        dots = dots + relative_bias\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "        out = torch.matmul(attn, v)\n",
        "\n",
        "        dots = torch.matmul(qx, kx.transpose(-1, -2)) * self.scale\n",
        "        dots = dots + relative_bias\n",
        "        attn = self.attend(dots)\n",
        "        out = torch.matmul(attn, out)\n",
        "\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class XTransformer(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(inp * 4)\n",
        "\n",
        "        self.ih, self.iw = image_size\n",
        "        self.downsample = downsample\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(inp)\n",
        "        self.layer_normx = nn.LayerNorm(inp)\n",
        "\n",
        "        if self.downsample:\n",
        "            self.pool1 = nn.MaxPool2d(3, 2, 1)\n",
        "            self.pool2 = nn.MaxPool2d(3, 2, 1)\n",
        "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
        "            self.pool1x = nn.MaxPool2d(3, 2, 1)\n",
        "            self.pool2x = nn.MaxPool2d(3, 2, 1)\n",
        "            self.projx = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
        "        self.attn = XAttention(inp, oup, image_size, heads, dim_head, dropout)\n",
        "        self.ff = FeedForward(oup, hidden_dim, dropout)\n",
        "        self.attnx = XAttention(inp, oup, image_size, heads, dim_head, dropout)\n",
        "        self.ffx = FeedForward(oup, hidden_dim, dropout)\n",
        "\n",
        "\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
        "            PreNorm(oup, self.ff, nn.LayerNorm),\n",
        "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "            )\n",
        "        self.ffx = nn.Sequential(\n",
        "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
        "            PreNorm(oup, self.ffx, nn.LayerNorm),\n",
        "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x[1]\n",
        "        x = x[0]\n",
        "\n",
        "        if self.downsample:\n",
        "            pool1 = self.pool2(x)\n",
        "            pool1 = rearrange(pool1, 'b c ih iw -> b (ih iw) c')\n",
        "            norm1 = self.layer_norm(pool1)\n",
        "            pool2 = self.pool2x(y)\n",
        "            pool2 = rearrange(pool2, 'b c ih iw -> b (ih iw) c')\n",
        "            norm2 = self.layer_normx(pool2)\n",
        "\n",
        "            attn1 = self.attn(norm1, norm2)\n",
        "            attn1 = rearrange(attn1, 'b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "            attn2 = self.attnx(norm2, norm1)\n",
        "            attn2 = rearrange(attn2, 'b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "\n",
        "            out1 = self.proj(self.pool1(x)) + attn1\n",
        "            out2 = self.projx(self.pool1x(y)) + attn2\n",
        "        else:\n",
        "            xx = rearrange(x, 'b c ih iw -> b (ih iw) c')\n",
        "            norm1 = self.layer_norm(xx)\n",
        "            yy = rearrange(y, 'b c ih iw -> b (ih iw) c')\n",
        "            norm2 = self.layer_normx(yy)\n",
        "\n",
        "            attn1 = self.attn(norm1, norm2)\n",
        "            attn1 = rearrange(attn1, 'b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "            attn2 = self.attnx(norm2, norm1)\n",
        "            attn2 = rearrange(attn2, 'b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "            out1 = x + attn1\n",
        "            out2 = y + attn2\n",
        "\n",
        "        out1 = out1 + self.ff(out1)\n",
        "        out2 = out2 + self.ffx(out2)\n",
        "        return [out1, out2]\n",
        "\n",
        "\n",
        "class CoAtXNet(nn.Module):\n",
        "    def __init__(self, image_size, in_channels, aux_channels, num_blocks, channels, block_types=['C', 'C', 'T', 'T']):\n",
        "        super().__init__()\n",
        "        ih, iw = image_size\n",
        "        block = {'C': XMBConv, 'T': XTransformer}\n",
        "\n",
        "        self.s0 = self._make_layer(\n",
        "            conv_3x3_bn, in_channels, channels[0], num_blocks[0], (ih // 2, iw // 2))\n",
        "        self.s0x = self._make_layer(\n",
        "            conv_3x3_bn, aux_channels, channels[0], num_blocks[0], (ih // 2, iw // 2))\n",
        "\n",
        "        self.s1 = self._make_layer(\n",
        "            block[block_types[0]], channels[0], channels[1], num_blocks[1], (ih // 4, iw // 4))\n",
        "        self.s2 = self._make_layer(\n",
        "            block[block_types[1]], channels[1], channels[2], num_blocks[2], (ih // 8, iw // 8))\n",
        "        self.s3 = self._make_layer(\n",
        "            block[block_types[2]], channels[2], channels[3], num_blocks[3], (ih // 16, iw // 16))\n",
        "        self.s4 = self._make_layer(\n",
        "            block[block_types[3]], channels[3], channels[4], num_blocks[4], (ih // 32, iw // 32))\n",
        "\n",
        "        self.pool = nn.AvgPool2d(ih // 32, 1)\n",
        "        hidden_dim = 1024\n",
        "        self.fc1 = nn.Linear(channels[-1] * 2, hidden_dim)\n",
        "        self.relu = nn.ReLU()  # Activation function between dense layers\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)  # Another dense layer\n",
        "        self.fc3 = nn.Linear(hidden_dim // 2, 6)  # Final output layer adjusted to target size\n",
        "\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        x = self.s0(x)\n",
        "        y = self.s0x(y)\n",
        "\n",
        "        xy = self.s1([x,y])\n",
        "        xy = self.s2(xy)\n",
        "        xy = self.s3(xy)\n",
        "        xy = self.s4(xy)\n",
        "\n",
        "        x = xy[0]\n",
        "        y = xy[1]\n",
        "\n",
        "        x = self.pool(x).view(-1, x.shape[1])\n",
        "        y = self.pool(y).view(-1, y.shape[1])\n",
        "\n",
        "        x = torch.cat((x,y), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, block, inp, oup, depth, image_size):\n",
        "        layers = nn.ModuleList([])\n",
        "        for i in range(depth):\n",
        "            if i == 0:\n",
        "                layers.append(block(inp, oup, image_size, downsample=True))\n",
        "            else:\n",
        "                layers.append(block(oup, oup, image_size))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "\n",
        "def coatxnet_0():\n",
        "    num_blocks = [2, 2, 3, 5, 2]            # L\n",
        "    channels = [64, 96, 192, 384, 768]      # D\n",
        "    return CoAtXNet((256, 256), 3, 1, num_blocks, channels, )\n",
        "\n",
        "\n",
        "def coatxnet_1():\n",
        "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
        "    channels = [64, 96, 192, 384, 768]      # D\n",
        "    return CoAtXNet((256, 256), 3, 1, num_blocks, channels, )\n",
        "\n",
        "\n",
        "def coatxnet_2():\n",
        "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
        "    channels = [128, 128, 256, 512, 1026]   # D\n",
        "    return CoAtXNet((256, 256), 3, 1, num_blocks, channels, )\n",
        "\n",
        "\n",
        "def coatxnet_3():\n",
        "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
        "    channels = [192, 192, 384, 768, 1536]   # D\n",
        "    return CoAtXNet((256, 256), 3, 1, num_blocks, channels, )\n",
        "\n",
        "\n",
        "def coatxnet_4():\n",
        "    num_blocks = [2, 2, 12, 28, 2]          # L\n",
        "    channels = [192, 192, 384, 768, 1536]   # D\n",
        "    return CoAtXNet((256, 256), 3, 1, num_blocks, channels, )\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    img = torch.randn(1, 3, 256, 256)\n",
        "    aux = torch.randn(1, 1, 256, 256)\n",
        "\n",
        "    net = coatxnet_0()\n",
        "    out = net(img, aux)\n",
        "    print(out.shape, count_parameters(net))\n",
        "\n",
        "    net = coatxnet_1()\n",
        "    out = net(img, aux)\n",
        "    print(out.shape, count_parameters(net))\n",
        "\n",
        "    net = coatxnet_2()\n",
        "    out = net(img, aux)\n",
        "    print(out.shape, count_parameters(net))\n",
        "\n",
        "    net = coatxnet_3()\n",
        "    out = net(img, aux)\n",
        "    print(out.shape, count_parameters(net))\n",
        "\n",
        "    net = coatxnet_4()\n",
        "    out = net(img, aux)\n",
        "    print(out.shape, count_parameters(net))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXDRGOzYIUMy",
        "outputId": "c5e0fcd1-53c4-4ec6-bd31-00c619a6de1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6]) 39113334\n",
            "torch.Size([1, 6]) 73447686\n",
            "torch.Size([1, 6]) 120817534\n",
            "torch.Size([1, 6]) 249078022\n",
            "torch.Size([1, 6]) 432611814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    root_dir = '/content/drive/MyDrive/fire'\n",
        "    dataset = FireDataset(root_dir=root_dir, transform=transform)\n",
        "\n",
        "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    num_epochs = 100  # Define the number of epochs\n",
        "\n",
        "    # Check for GPU availability\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
        "        print(f'Fold {fold + 1}')\n",
        "\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "        train_loader = DataLoader(dataset, batch_size=16, sampler=train_sampler, num_workers=4)\n",
        "        val_loader = DataLoader(dataset, batch_size=16, sampler=val_sampler, num_workers=4)\n",
        "\n",
        "        model = coatxnet_0().to(device)  # Move model to GPU\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=4, factor=0.1)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            for color_images, depth_images, poses in tqdm(train_loader):\n",
        "                color_images, depth_images, poses = color_images.to(device), depth_images.to(device), poses.to(device)  # Move data to GPU\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(color_images, depth_images)\n",
        "                loss = ModifiedDSACLoss(poses, outputs)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            # Validation loop\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for color_images, depth_images, poses in val_loader:\n",
        "                    color_images, depth_images, poses = color_images.to(device), depth_images.to(device), poses.to(device)  # Move data to GPU\n",
        "                    outputs = model(color_images, depth_images)\n",
        "                    loss = ModifiedDSACLoss(poses, outputs)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            # Step the scheduler\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            print(f'Epoch {epoch + 1}, Train Loss: {train_loss / len(train_loader)}, Val Loss: {val_loss / len(val_loader)}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0MRGzvatIbAc",
        "outputId": "0b49c638-de64-40e7-cbeb-b16bebeb269e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 4003\n",
            "Number of processed poses: 4000\n",
            "Using device: cuda\n",
            "Fold 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:57<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 1.993081813863203, Val Loss: 1.342734135745442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Train Loss: 0.9658291913098104, Val Loss: 0.8676206565434796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Train Loss: 0.6989103474238176, Val Loss: 0.6251437482967169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Train Loss: 0.5218564901809246, Val Loss: 0.5353583551580716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Train Loss: 0.47359679377628566, Val Loss: 0.3963516964535389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Train Loss: 0.4002213074400416, Val Loss: 0.3980103263006832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Train Loss: 0.3714294430822656, Val Loss: 0.31901823638348614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Train Loss: 0.37567142263951275, Val Loss: 0.41692652170206235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Train Loss: 0.3490380869266771, Val Loss: 0.3640785355383345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Train Loss: 0.3196897188636759, Val Loss: 0.35211216782080534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Train Loss: 0.2924918472776177, Val Loss: 0.36402951486870466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Train Loss: 0.2852795043122164, Val Loss: 0.29195322906530025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Train Loss: 0.2906443618856737, Val Loss: 0.2704663847746235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Train Loss: 0.2764799658818994, Val Loss: 0.29301771710626917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Train Loss: 0.25589874241065264, Val Loss: 0.3417427917358372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Train Loss: 0.24686881488549342, Val Loss: 0.28533759292339755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Train Loss: 0.23762950547745323, Val Loss: 0.2537030939050054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18, Train Loss: 0.23330379756050834, Val Loss: 0.2934001698328361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19, Train Loss: 0.2517409859986225, Val Loss: 0.2737914588499213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Train Loss: 0.23013957228681828, Val Loss: 0.29402469039640006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21, Train Loss: 0.2359200574850113, Val Loss: 0.2199113802446532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22, Train Loss: 0.22428912503266538, Val Loss: 0.25954228140969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23, Train Loss: 0.21367057965250866, Val Loss: 0.21425162465194791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24, Train Loss: 0.20972587798832426, Val Loss: 0.2388325921145366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25, Train Loss: 0.2201044695925851, Val Loss: 0.20651749247688514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26, Train Loss: 0.20449698102541883, Val Loss: 0.20830642411844968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27, Train Loss: 0.1854488479469725, Val Loss: 0.2296237209787755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28, Train Loss: 0.19679306952951425, Val Loss: 0.2627188230917013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29, Train Loss: 0.18635946726460573, Val Loss: 0.2267239670553307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30, Train Loss: 0.19041061615504365, Val Loss: 0.21375600145215812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31, Train Loss: 0.10609335090851339, Val Loss: 0.10304507703529485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32, Train Loss: 0.07590008388287368, Val Loss: 0.10393898522327094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33, Train Loss: 0.06849639585851264, Val Loss: 0.09298749940944873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34, Train Loss: 0.06320208568505167, Val Loss: 0.10071684801213739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35, Train Loss: 0.06516647013169892, Val Loss: 0.08934462650201025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36, Train Loss: 0.058511080965204625, Val Loss: 0.08968720868974644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37, Train Loss: 0.05586276073274659, Val Loss: 0.08942123949896948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38, Train Loss: 0.05123415451348163, Val Loss: 0.08537135678727908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39, Train Loss: 0.053600711801501694, Val Loss: 0.0914419219188082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40, Train Loss: 0.050306716097115525, Val Loss: 0.08810283674984634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41, Train Loss: 0.05138737884284542, Val Loss: 0.09413731636673596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42, Train Loss: 0.049518169076566226, Val Loss: 0.09221198389882351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43, Train Loss: 0.047928958023687816, Val Loss: 0.08957258236612368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44, Train Loss: 0.040228569963272405, Val Loss: 0.0819921978770416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45, Train Loss: 0.03664603921198504, Val Loss: 0.08177976414612113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46, Train Loss: 0.036029112830419104, Val Loss: 0.08108790803523096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47, Train Loss: 0.036356574069241, Val Loss: 0.08590253675021442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48, Train Loss: 0.03531358744276075, Val Loss: 0.08444169286490705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49, Train Loss: 0.03473574723663344, Val Loss: 0.08304508347640883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50, Train Loss: 0.03521336427091646, Val Loss: 0.08441893229803071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 51, Train Loss: 0.034095362612645654, Val Loss: 0.0921290936057079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 52, Train Loss: 0.03420557604355843, Val Loss: 0.08769997796257727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 53, Train Loss: 0.033861859940441476, Val Loss: 0.0883238790022642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 54, Train Loss: 0.03321892847650239, Val Loss: 0.08636812211821603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 55, Train Loss: 0.03304571201169126, Val Loss: 0.08688399399052896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 56, Train Loss: 0.03250617886805878, Val Loss: 0.08848238178189258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 57, Train Loss: 0.032950720485830524, Val Loss: 0.08525164992022022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 58, Train Loss: 0.03328098749833187, Val Loss: 0.08387427329635161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 59, Train Loss: 0.03277448180586397, Val Loss: 0.08261084336711381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60, Train Loss: 0.033032194197522784, Val Loss: 0.08497118037378437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 61, Train Loss: 0.03268516572174336, Val Loss: 0.08559771737193528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 62, Train Loss: 0.0323632280385949, Val Loss: 0.08491846867818338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 63, Train Loss: 0.0332257199608498, Val Loss: 0.08325519242233949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 64, Train Loss: 0.03243675097173406, Val Loss: 0.0863402939974001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 65, Train Loss: 0.03287939306210186, Val Loss: 0.08562019890632727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 66, Train Loss: 0.033142621614470784, Val Loss: 0.08274469177555285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 67, Train Loss: 0.03277209845026597, Val Loss: 0.08252571427305382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 68, Train Loss: 0.03212446193995691, Val Loss: 0.08396444337845976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 69, Train Loss: 0.03251794536220134, Val Loss: 0.0939030239655982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70, Train Loss: 0.03240819663988994, Val Loss: 0.08409225073513431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 71, Train Loss: 0.03336278001725261, Val Loss: 0.08156803157646608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 72, Train Loss: 0.032415017988924134, Val Loss: 0.08736801877397067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 73, Train Loss: 0.03256668335260484, Val Loss: 0.08532750975007886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 74, Train Loss: 0.032734897776716794, Val Loss: 0.08462067024701143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 75, Train Loss: 0.03274727325242893, Val Loss: 0.08610068802337123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 76, Train Loss: 0.03291786886134636, Val Loss: 0.08688459577987866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 77, Train Loss: 0.03265877287095932, Val Loss: 0.08658923176238435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 78, Train Loss: 0.03279096791466047, Val Loss: 0.08505740182855465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 79, Train Loss: 0.03242267338098398, Val Loss: 0.08551828428382896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80, Train Loss: 0.03295913569043028, Val Loss: 0.08551890742097956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:58<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 81, Train Loss: 0.03285468637969745, Val Loss: 0.08354877143805188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 16/200 [00:11<02:08,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b4134b0f95bb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Validation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chess"
      ],
      "metadata": {
        "id": "N1MKHxiXnFqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfgofs-I6sS6",
        "outputId": "a713a16e-2dc5-4dfe-8495-7bc5456905bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transforms3d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsJ-1bwB6vfe",
        "outputId": "8665238a-6109-4d54-8b24-f7861a78a75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transforms3d\n",
            "  Downloading transforms3d-0.4.2-py3-none-any.whl (1.4 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.4 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from transforms3d) (1.25.2)\n",
            "Installing collected packages: transforms3d\n",
            "Successfully installed transforms3d-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXB5lNVh6yMN",
        "outputId": "b5713fa9-2128-4535-bf57-2d3f6d553f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomRotation, ToTensor\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "from torchvision.utils import make_grid\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "from sklearn.model_selection import KFold\n",
        "from tqdm import tqdm\n",
        "import transforms3d.quaternions as txq\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "oUG4Im8p61Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ModifiedDSACLoss(y_true, y_pred, weight_translation_initial=1.0, weight_quaternion_initial=1.0):\n",
        "    # Split the true and predicted values into delta positions and logarithmic quaternions\n",
        "    true_positions, true_log_quaternions = torch.split(y_true, [3, 3], dim=-1)\n",
        "    pred_positions, pred_log_quaternions = torch.split(y_pred, [3, 3], dim=-1)\n",
        "\n",
        "    # Calculate the translation error (L2 distance)\n",
        "    t_error = torch.sqrt(torch.sum((true_positions - pred_positions) ** 2, dim=-1))\n",
        "    mean_t_error = torch.mean(t_error)\n",
        "\n",
        "    # Calculate the logarithmic quaternion error (L2 distance)\n",
        "    q_error = torch.sqrt(torch.sum((true_log_quaternions - pred_log_quaternions) ** 2, dim=-1))\n",
        "    mean_q_error = torch.mean(q_error)\n",
        "\n",
        "    # Dynamic weight adjustment based on the error ratio\n",
        "    error_ratio = mean_t_error / (mean_q_error + 1e-8)  # Adding a small epsilon to avoid division by zero\n",
        "    weight_translation = weight_translation_initial * error_ratio\n",
        "    weight_quaternion = weight_quaternion_initial / error_ratio\n",
        "\n",
        "    # Apply dynamic weights to the respective errors\n",
        "    weighted_t_error = weight_translation * mean_t_error\n",
        "    weighted_q_error = weight_quaternion * mean_q_error\n",
        "\n",
        "    # Final combined weighted error\n",
        "    combined_weighted_error = weighted_t_error + weighted_q_error\n",
        "\n",
        "    return combined_weighted_error\n"
      ],
      "metadata": {
        "id": "txt6q9WA66Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def qlog(q):\n",
        "    q = q / np.linalg.norm(q)\n",
        "    sinhalftheta = np.linalg.norm(q[1:])\n",
        "    coshalftheta = q[0]\n",
        "    r = np.arctan2(sinhalftheta, coshalftheta)\n",
        "    if sinhalftheta > 1e-6:\n",
        "        qlog = r * q[1:] / sinhalftheta\n",
        "    else:\n",
        "        qlog = np.zeros(3)\n",
        "    return qlog\n",
        "\n",
        "def process_poses(poses_in, mean_t, std_t, align_R, align_t, align_s):\n",
        "    poses_out = np.zeros((len(poses_in), 6))\n",
        "    poses_out[:, 0:3] = poses_in[:, [3, 7, 11]]  # Translation components\n",
        "\n",
        "    # Align and process rotation\n",
        "    for i in range(len(poses_out)):\n",
        "        R = poses_in[i].reshape((3, 4))[:3, :3]\n",
        "        q = txq.mat2quat(np.dot(align_R, R))\n",
        "        q *= np.sign(q[0])  # Constrain to hemisphere\n",
        "        q = qlog(q)\n",
        "        poses_out[i, 3:] = q\n",
        "        t = poses_out[i, :3] - align_t\n",
        "        poses_out[i, :3] = align_s * np.dot(align_R, t[:, np.newaxis]).squeeze()\n",
        "\n",
        "    # Normalize translation\n",
        "    poses_out[:, :3] -= mean_t\n",
        "    poses_out[:, :3] /= std_t\n",
        "\n",
        "    return poses_out\n",
        "\n",
        "def load_and_process_poses(data_dir, seqs, train=True, real=False, vo_lib='orbslam'):\n",
        "    ps = {}\n",
        "    vo_stats = {}\n",
        "    all_poses = []\n",
        "    for seq in seqs:\n",
        "        seq_dir = os.path.join(data_dir, seq)\n",
        "        p_filenames = [n for n in os.listdir(seq_dir) if n.find('pose') >= 0]\n",
        "        if real:\n",
        "            pose_file = os.path.join(data_dir, '{:s}_poses'.format(vo_lib), seq)\n",
        "            pss = np.loadtxt(pose_file)\n",
        "            frame_idx = pss[:, 0].astype(int)\n",
        "            if vo_lib == 'libviso2':\n",
        "                frame_idx -= 1\n",
        "            ps[seq] = pss[:, 1:13]\n",
        "            vo_stats_filename = os.path.join(seq_dir, '{:s}_vo_stats.pkl'.format(vo_lib))\n",
        "            with open(vo_stats_filename, 'rb') as f:\n",
        "                vo_stats[seq] = pickle.load(f)\n",
        "        else:\n",
        "            frame_idx = np.array(range(len(p_filenames)), dtype=int)\n",
        "            pss = [np.loadtxt(os.path.join(seq_dir, 'frame-{:06d}.pose.txt'.format(i))).flatten()[:12] for i in frame_idx if os.path.exists(os.path.join(seq_dir, 'frame-{:06d}.pose.txt'.format(i)))]\n",
        "            ps[seq] = np.asarray(pss)\n",
        "            vo_stats[seq] = {'R': np.eye(3), 't': np.zeros(3), 's': 1}\n",
        "\n",
        "        all_poses.append(ps[seq])\n",
        "\n",
        "    all_poses = np.vstack(all_poses)\n",
        "    pose_stats_filename = os.path.join(data_dir, 'pose_stats.txt')\n",
        "    if train and not real:\n",
        "        mean_t = np.mean(all_poses[:, [3, 7, 11]], axis=0)\n",
        "        std_t = np.std(all_poses[:, [3, 7, 11]], axis=0)\n",
        "        np.savetxt(pose_stats_filename, np.vstack((mean_t, std_t)), fmt='%8.7f')\n",
        "    else:\n",
        "        mean_t, std_t = np.loadtxt(pose_stats_filename)\n",
        "\n",
        "    # Process and normalize poses\n",
        "    processed_poses = []\n",
        "    for seq in seqs:\n",
        "        pss = process_poses(poses_in=ps[seq], mean_t=mean_t, std_t=std_t,\n",
        "                            align_R=vo_stats[seq]['R'], align_t=vo_stats[seq]['t'],\n",
        "                            align_s=vo_stats[seq]['s'])\n",
        "        processed_poses.append(pss)\n",
        "\n",
        "    return np.vstack(processed_poses)\n",
        "\n",
        "# Define the transformation for the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Dataset class\n",
        "class FireDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.seqs = [seq for seq in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, seq))]\n",
        "        self.samples = self._load_samples()\n",
        "        self.processed_poses = self._load_processed_poses()\n",
        "\n",
        "        # Debugging prints\n",
        "        print(f\"Number of samples: {len(self.samples)}\")\n",
        "        print(f\"Number of processed poses: {self.processed_poses.shape[0]}\")\n",
        "\n",
        "        # Ensure consistency between samples and processed poses\n",
        "        min_length = min(len(self.samples), self.processed_poses.shape[0])\n",
        "        self.samples = self.samples[:min_length]\n",
        "        self.processed_poses = self.processed_poses[:min_length]\n",
        "\n",
        "    def _load_samples(self):\n",
        "        samples = []\n",
        "        for seq_folder in self.seqs:\n",
        "            seq_path = os.path.join(self.root_dir, seq_folder)\n",
        "            color_files = sorted([f for f in os.listdir(seq_path) if f.endswith('.color.png')])\n",
        "            depth_files = sorted([f for f in os.listdir(seq_path) if f.endswith('.depth.png')])\n",
        "            pose_files = sorted([f for f in os.listdir(seq_path) if f.endswith('.pose.txt')])\n",
        "\n",
        "            for color_file, depth_file, pose_file in zip(color_files, depth_files, pose_files):\n",
        "                samples.append((os.path.join(seq_path, color_file),\n",
        "                                os.path.join(seq_path, depth_file),\n",
        "                                os.path.join(seq_path, pose_file)))\n",
        "        return samples\n",
        "\n",
        "    def _load_processed_poses(self):\n",
        "        return load_and_process_poses(self.root_dir, self.seqs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self.processed_poses):\n",
        "            raise IndexError(f\"Index {idx} out of bounds for processed poses of size {len(self.processed_poses)}\")\n",
        "\n",
        "        color_path, depth_path, pose_path = self.samples[idx]\n",
        "\n",
        "        color_image = cv2.imread(color_path, cv2.IMREAD_COLOR)\n",
        "        depth_image = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n",
        "        pose_matrix = self.processed_poses[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            color_image = self.transform(color_image)\n",
        "            depth_image = (depth_image / depth_image.max() * 255).astype(np.uint8)\n",
        "            depth_image = self.transform(depth_image)\n",
        "\n",
        "        return color_image, depth_image, pose_matrix\n"
      ],
      "metadata": {
        "id": "CgwjQBhB68nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_3x3_bn(inp, oup, image_size, downsample=False):\n",
        "    stride = 1 if downsample == False else 2\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.GELU()\n",
        "    )\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn, norm):\n",
        "        super().__init__()\n",
        "        self.norm = norm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class SE(nn.Module):\n",
        "    def __init__(self, inp, oup, expansion=0.25):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(oup, int(inp * expansion), bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(inp * expansion), oup, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class XMBConv(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, downsample=False, expansion=4):\n",
        "        super().__init__()\n",
        "        self.downsample = downsample\n",
        "        stride = 1 if self.downsample == False else 2\n",
        "        hidden_dim = int(inp * expansion)\n",
        "\n",
        "        if self.downsample:\n",
        "            self.pool = nn.MaxPool2d(3, 2, 1)\n",
        "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
        "            self.poolx = nn.MaxPool2d(3, 2, 1)\n",
        "            self.projx = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
        "\n",
        "        if expansion == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n",
        "                          1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "            self.convx = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n",
        "                          1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                # down-sample in the first conv\n",
        "                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n",
        "                          groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                SE(inp, hidden_dim),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "            self.convx = nn.Sequential(\n",
        "                # pw\n",
        "                # down-sample in the first conv\n",
        "                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n",
        "                          groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                SE(inp, hidden_dim),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n",
        "        self.convx = PreNorm(inp, self.convx, nn.BatchNorm2d)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x[1]\n",
        "        x = x[0]\n",
        "\n",
        "        if self.downsample:\n",
        "            out1 = self.proj(self.pool(x)) + self.conv(x)\n",
        "            out2 = self.projx(self.poolx(y)) + self.convx(y)\n",
        "        else:\n",
        "            out1 = x + self.conv(x)\n",
        "            out2 = y + self.convx(y)\n",
        "        return [out1, out2]\n",
        "\n",
        "class XAttention(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        project_out = not (heads == 1 and dim_head == inp)\n",
        "\n",
        "        self.ih, self.iw = image_size\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        # parameter table of relative position bias\n",
        "        self.relative_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
        "\n",
        "        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
        "        coords = torch.flatten(torch.stack(coords), 1)\n",
        "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
        "\n",
        "        relative_coords[0] += self.ih - 1\n",
        "        relative_coords[1] += self.iw - 1\n",
        "        relative_coords[0] *= 2 * self.iw - 1\n",
        "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
        "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
        "        self.register_buffer(\"relative_index\", relative_index)\n",
        "\n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
        "        self.to_kx = nn.Linear(inp, inner_dim * 1, bias=False)\n",
        "        self.to_qx = nn.Linear(inp, inner_dim * 1, bias=False)\n",
        "\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, oup),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        kx = self.to_kx(y)\n",
        "        qx = self.to_qx(x)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(\n",
        "            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
        "        kx = rearrange(kx, 'b n (h d) -> b h n d', h=self.heads)\n",
        "        qx = rearrange(qx, 'b n (h d) -> b h n d', h=self.heads)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "        # Use \"gather\" for more efficiency on GPUs\n",
        "        relative_bias = self.relative_bias_table.gather(\n",
        "            0, self.relative_index.repeat(1, self.heads))\n",
        "        relative_bias = rearrange(\n",
        "            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
        "        dots = dots + relative_bias\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "        out = torch.matmul(attn, v)\n",
        "\n",
        "        dots = torch.matmul(qx, kx.transpose(-1, -2)) * self.scale\n",
        "        dots = dots + relative_bias\n",
        "        attn = self.attend(dots)\n",
        "        out = torch.matmul(attn, out)\n",
        "\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out = self.to_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class XTransformer(nn.Module):\n",
        "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n",
        "        super().__init__()\n",
        "        hidden_dim = int(inp * 4)\n",
        "\n",
        "        self.ih, self.iw = image_size\n",
        "        self.downsample = downsample\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(inp)\n",
        "        self.layer_normx = nn.LayerNorm(inp)\n",
        "\n",
        "        if self.downsample:\n",
        "            self.pool1 = nn.MaxPool2d(3, 2, 1)\n",
        "            self.pool2 = nn.MaxPool2d(3, 2, 1)\n",
        "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
        "            self.pool1x = nn.MaxPool2d(3, 2, 1)\n",
        "            self.pool2x = nn.MaxPool2d(3, 2, 1)\n",
        "            self.projx = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
        "        self.attn = XAttention(inp, oup, image_size, heads, dim_head, dropout)\n",
        "        self.ff = FeedForward(oup, hidden_dim, dropout)\n",
        "        self.attnx = XAttention(inp, oup, image_size, heads, dim_head, dropout)\n",
        "        self.ffx = FeedForward(oup, hidden_dim, dropout)\n",
        "\n",
        "\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
        "            PreNorm(oup, self.ff, nn.LayerNorm),\n",
        "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "            )\n",
        "        self.ffx = nn.Sequential(\n",
        "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
        "            PreNorm(oup, self.ffx, nn.LayerNorm),\n",
        "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x[1]\n",
        "        x = x[0]\n",
        "\n",
        "        if self.downsample:\n",
        "            pool1 = self.pool2(x)\n",
        "            pool1 = rearrange(pool1, 'b c ih iw -> b (ih iw) c')\n",
        "            norm1 = self.layer_norm(pool1)\n",
        "            pool2 = self.pool2x(y)\n",
        "            pool2 = rearrange(pool2, 'b c ih iw -> b (ih iw) c')\n",
        "            norm2 = self.layer_normx(pool2)\n",
        "\n",
        "            attn1 = self.attn(norm1, norm2)\n",
        "            attn1 = rearrange(attn1, 'b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "            attn2 = self.attnx(norm2, norm1)\n",
        "            attn2 = rearrange(attn2, 'b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "\n",
        "            out1 = self.proj(self.pool1(x)) + attn1\n",
        "            out2 = self.projx(self.pool1x(y)) + attn2\n",
        "        else:\n",
        "            xx = rearrange(x, 'b c ih iw -> b (ih iw) c')\n",
        "            norm1 = self.layer_norm(xx)\n",
        "            yy = rearrange(y, 'b c ih iw -> b (ih iw) c')\n",
        "            norm2 = self.layer_normx(yy)\n",
        "\n",
        "            attn1 = self.attn(norm1, norm2)\n",
        "            attn1 = rearrange(attn1, 'b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "            attn2 = self.attnx(norm2, norm1)\n",
        "            attn2 = rearrange(attn2, 'b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
        "            out1 = x + attn1\n",
        "            out2 = y + attn2\n",
        "\n",
        "        out1 = out1 + self.ff(out1)\n",
        "        out2 = out2 + self.ffx(out2)\n",
        "        return [out1, out2]\n",
        "\n",
        "\n",
        "class CoAtXNet(nn.Module):\n",
        "    def __init__(self, image_size, in_channels, aux_channels, num_blocks, channels, block_types=['C', 'C', 'T', 'T']):\n",
        "        super().__init__()\n",
        "        ih, iw = image_size\n",
        "        block = {'C': XMBConv, 'T': XTransformer}\n",
        "\n",
        "        self.s0 = self._make_layer(\n",
        "            conv_3x3_bn, in_channels, channels[0], num_blocks[0], (ih // 2, iw // 2))\n",
        "        self.s0x = self._make_layer(\n",
        "            conv_3x3_bn, aux_channels, channels[0], num_blocks[0], (ih // 2, iw // 2))\n",
        "\n",
        "        self.s1 = self._make_layer(\n",
        "            block[block_types[0]], channels[0], channels[1], num_blocks[1], (ih // 4, iw // 4))\n",
        "        self.s2 = self._make_layer(\n",
        "            block[block_types[1]], channels[1], channels[2], num_blocks[2], (ih // 8, iw // 8))\n",
        "        self.s3 = self._make_layer(\n",
        "            block[block_types[2]], channels[2], channels[3], num_blocks[3], (ih // 16, iw // 16))\n",
        "        self.s4 = self._make_layer(\n",
        "            block[block_types[3]], channels[3], channels[4], num_blocks[4], (ih // 32, iw // 32))\n",
        "\n",
        "        self.pool = nn.AvgPool2d(ih // 32, 1)\n",
        "        hidden_dim = 1024\n",
        "        self.fc1 = nn.Linear(channels[-1] * 2, hidden_dim)\n",
        "        self.relu = nn.ReLU()  # Activation function between dense layers\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)  # Another dense layer\n",
        "        self.fc3 = nn.Linear(hidden_dim // 2, 6)  # Final output layer adjusted to target size\n",
        "\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        x = self.s0(x)\n",
        "        y = self.s0x(y)\n",
        "\n",
        "        xy = self.s1([x,y])\n",
        "        xy = self.s2(xy)\n",
        "        xy = self.s3(xy)\n",
        "        xy = self.s4(xy)\n",
        "\n",
        "        x = xy[0]\n",
        "        y = xy[1]\n",
        "\n",
        "        x = self.pool(x).view(-1, x.shape[1])\n",
        "        y = self.pool(y).view(-1, y.shape[1])\n",
        "\n",
        "        x = torch.cat((x,y), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, block, inp, oup, depth, image_size):\n",
        "        layers = nn.ModuleList([])\n",
        "        for i in range(depth):\n",
        "            if i == 0:\n",
        "                layers.append(block(inp, oup, image_size, downsample=True))\n",
        "            else:\n",
        "                layers.append(block(oup, oup, image_size))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "\n",
        "def coatxnet_0():\n",
        "    num_blocks = [2, 2, 3, 5, 2]            # L\n",
        "    channels = [64, 96, 192, 384, 768]      # D\n",
        "    return CoAtXNet((256, 256), 3, 1, num_blocks, channels, )\n",
        "\n",
        "\n",
        "def coatxnet_1():\n",
        "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
        "    channels = [64, 96, 192, 384, 768]      # D\n",
        "    return CoAtXNet((256, 256), 3, 1, num_blocks, channels, )\n",
        "\n",
        "\n",
        "def coatxnet_2():\n",
        "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
        "    channels = [128, 128, 256, 512, 1026]   # D\n",
        "    return CoAtXNet((256, 256), 3, 1, num_blocks, channels, )\n",
        "\n",
        "\n",
        "def coatxnet_3():\n",
        "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
        "    channels = [192, 192, 384, 768, 1536]   # D\n",
        "    return CoAtXNet((256, 256), 3, 1, num_blocks, channels, )\n",
        "\n",
        "\n",
        "def coatxnet_4():\n",
        "    num_blocks = [2, 2, 12, 28, 2]          # L\n",
        "    channels = [192, 192, 384, 768, 1536]   # D\n",
        "    return CoAtXNet((256, 256), 3, 1, num_blocks, channels, )\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    img = torch.randn(1, 3, 256, 256)\n",
        "    aux = torch.randn(1, 1, 256, 256)\n",
        "\n",
        "    net = coatxnet_0()\n",
        "    out = net(img, aux)\n",
        "    print(out.shape, count_parameters(net))\n",
        "\n",
        "    net = coatxnet_1()\n",
        "    out = net(img, aux)\n",
        "    print(out.shape, count_parameters(net))\n",
        "\n",
        "    net = coatxnet_2()\n",
        "    out = net(img, aux)\n",
        "    print(out.shape, count_parameters(net))\n",
        "\n",
        "    net = coatxnet_3()\n",
        "    out = net(img, aux)\n",
        "    print(out.shape, count_parameters(net))\n",
        "\n",
        "    net = coatxnet_4()\n",
        "    out = net(img, aux)\n",
        "    print(out.shape, count_parameters(net))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B80f07B36_zE",
        "outputId": "06cb4ffd-a539-4db9-d5d3-1875f02dfa9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6]) 39113334\n",
            "torch.Size([1, 6]) 73447686\n",
            "torch.Size([1, 6]) 120817534\n",
            "torch.Size([1, 6]) 249078022\n",
            "torch.Size([1, 6]) 432611814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    root_dir = '/content/drive/MyDrive/Chess'\n",
        "    dataset = FireDataset(root_dir=root_dir, transform=transform)\n",
        "\n",
        "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    num_epochs = 100  # Define the number of epochs\n",
        "\n",
        "    # Check for GPU availability\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
        "        print(f'Fold {fold + 1}')\n",
        "\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "        train_loader = DataLoader(dataset, batch_size=16, sampler=train_sampler, num_workers=4)\n",
        "        val_loader = DataLoader(dataset, batch_size=16, sampler=val_sampler, num_workers=4)\n",
        "\n",
        "        model = coatxnet_0().to(device)  # Move model to GPU\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=4, factor=0.1)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            for color_images, depth_images, poses in tqdm(train_loader):\n",
        "                color_images, depth_images, poses = color_images.to(device), depth_images.to(device), poses.to(device)  # Move data to GPU\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(color_images, depth_images)\n",
        "                loss = ModifiedDSACLoss(poses, outputs)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            # Validation loop\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for color_images, depth_images, poses in val_loader:\n",
        "                    color_images, depth_images, poses = color_images.to(device), depth_images.to(device), poses.to(device)  # Move data to GPU\n",
        "                    outputs = model(color_images, depth_images)\n",
        "                    loss = ModifiedDSACLoss(poses, outputs)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            # Step the scheduler\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            print(f'Epoch {epoch + 1}, Train Loss: {train_loss / len(train_loader)}, Val Loss: {val_loss / len(val_loader)}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ukYtj4cM7ELb",
        "outputId": "fcff4bc1-e32e-451c-f85d-4b0495418415"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples: 6003\n",
            "Number of processed poses: 6000\n",
            "Using device: cuda\n",
            "Fold 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [02:54<00:00,  1.72it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 1.7983068812204634, Val Loss: 1.4698871979458124\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Train Loss: 0.9421943558084785, Val Loss: 0.621728616601762\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Train Loss: 0.6453091052652233, Val Loss: 0.47830394957867695\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Train Loss: 0.5295329828460815, Val Loss: 0.5127048591807912\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Train Loss: 0.44878955994185576, Val Loss: 0.5002289101562173\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, Train Loss: 0.40433002019266556, Val Loss: 0.38171451737208967\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7, Train Loss: 0.37682791260871634, Val Loss: 0.46488227879369715\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8, Train Loss: 0.3375694711015532, Val Loss: 0.32010125273413054\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9, Train Loss: 0.3236744967858662, Val Loss: 0.3064340718462233\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10, Train Loss: 0.3152138344676683, Val Loss: 0.31025875052596935\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11, Train Loss: 0.2903263163669485, Val Loss: 0.34930042338157796\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12, Train Loss: 0.2771455389924778, Val Loss: 0.32009038828569203\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13, Train Loss: 0.2766239274392061, Val Loss: 0.2816841714004821\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14, Train Loss: 0.28384906457551684, Val Loss: 0.2899942185679435\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15, Train Loss: 0.2672019974024281, Val Loss: 0.32893296217596024\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16, Train Loss: 0.24190573395040066, Val Loss: 0.27922463266839714\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17, Train Loss: 0.23784741596224204, Val Loss: 0.2665744481486065\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18, Train Loss: 0.2364740354158783, Val Loss: 0.2717955419074725\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19, Train Loss: 0.22966669940638634, Val Loss: 0.286048843583338\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20, Train Loss: 0.2452580335200247, Val Loss: 0.24978358723181732\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21, Train Loss: 0.22208745801344013, Val Loss: 0.28093981285650726\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22, Train Loss: 0.22738439782185316, Val Loss: 0.2507329718137206\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23, Train Loss: 0.2277851046847933, Val Loss: 0.2662957851023282\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24, Train Loss: 0.2097722825143312, Val Loss: 0.18204084159957337\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25, Train Loss: 0.20902300397036847, Val Loss: 0.2449037877469742\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26, Train Loss: 0.20628602131351195, Val Loss: 0.21894727092730434\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27, Train Loss: 0.19574622298211422, Val Loss: 0.20710562725831907\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28, Train Loss: 0.2096861280418125, Val Loss: 0.22225067662837816\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29, Train Loss: 0.19787219847965934, Val Loss: 0.1932280211849779\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30, Train Loss: 0.11657965745547157, Val Loss: 0.12061688874799312\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31, Train Loss: 0.08611123544110139, Val Loss: 0.10679693840827414\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32, Train Loss: 0.07501723822659047, Val Loss: 0.09911411896525857\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33, Train Loss: 0.06829326855165758, Val Loss: 0.10010879600071208\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34, Train Loss: 0.06424999502313687, Val Loss: 0.09418810316499163\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35, Train Loss: 0.05967541590167159, Val Loss: 0.10237924102935016\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36, Train Loss: 0.0592864251222336, Val Loss: 0.0998707430578007\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37, Train Loss: 0.056142614318329676, Val Loss: 0.09967093110245899\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38, Train Loss: 0.05504282905962176, Val Loss: 0.09499499764996822\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39, Train Loss: 0.052190239585114234, Val Loss: 0.09741960331192379\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40, Train Loss: 0.04506234755249034, Val Loss: 0.09165270497516992\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41, Train Loss: 0.04258542775758136, Val Loss: 0.09090732329613284\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42, Train Loss: 0.0419270329048948, Val Loss: 0.09237736261048883\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43, Train Loss: 0.0409978322385846, Val Loss: 0.09175909764266278\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44, Train Loss: 0.04060836570261072, Val Loss: 0.0944516489216771\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45, Train Loss: 0.04054346183201674, Val Loss: 0.09243858590243616\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46, Train Loss: 0.03934732701928314, Val Loss: 0.09594620462516788\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47, Train Loss: 0.038965218373618916, Val Loss: 0.09299864035541411\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48, Train Loss: 0.038903577993509984, Val Loss: 0.0947981290370632\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49, Train Loss: 0.03846752406118333, Val Loss: 0.0932763530198907\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.65it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50, Train Loss: 0.03883842599549363, Val Loss: 0.09166460834471213\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 51, Train Loss: 0.03892337336567418, Val Loss: 0.09352934731650298\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 52, Train Loss: 0.03875948257969771, Val Loss: 0.09425068324497454\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 53, Train Loss: 0.03847804934818474, Val Loss: 0.09735323239567086\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.65it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 54, Train Loss: 0.038865883784084614, Val Loss: 0.09491084218989614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 55, Train Loss: 0.038573385501272914, Val Loss: 0.09324473874595204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:02<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 56, Train Loss: 0.038861309975708726, Val Loss: 0.09432581787635395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:02<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 57, Train Loss: 0.038752527312994285, Val Loss: 0.09164700935142493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 58, Train Loss: 0.038937592787670505, Val Loss: 0.09431615262441444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 59, Train Loss: 0.038577241060484976, Val Loss: 0.0904695927998115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60, Train Loss: 0.03865823851288361, Val Loss: 0.09264367550349277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 61, Train Loss: 0.038487190739091404, Val Loss: 0.09382611500906297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 62, Train Loss: 0.03882631533905583, Val Loss: 0.09367172937260865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 63, Train Loss: 0.03875736012357891, Val Loss: 0.09153322261124316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 64, Train Loss: 0.03864737864728714, Val Loss: 0.09558250196593691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 65, Train Loss: 0.038791682140708, Val Loss: 0.09338097344607302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 66, Train Loss: 0.0383913537124795, Val Loss: 0.0960997957864358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 67, Train Loss: 0.03878554185190682, Val Loss: 0.0915024559700136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 68, Train Loss: 0.03874986984652797, Val Loss: 0.09152825627449503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 69, Train Loss: 0.0389417280916881, Val Loss: 0.0947799199651748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70, Train Loss: 0.03794767185616998, Val Loss: 0.09383560374319139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:02<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 71, Train Loss: 0.038870321080480726, Val Loss: 0.0925121464245651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:02<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 72, Train Loss: 0.03816235509890276, Val Loss: 0.09293557604426486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 73, Train Loss: 0.038292031135935586, Val Loss: 0.09286262149707225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 74, Train Loss: 0.03916591256709015, Val Loss: 0.09293019448695947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 75, Train Loss: 0.038803353310441185, Val Loss: 0.0932691456649693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 76, Train Loss: 0.038471819164891945, Val Loss: 0.09443484264506707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 77, Train Loss: 0.038471983438920554, Val Loss: 0.09327671034705852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 78, Train Loss: 0.03843073467175622, Val Loss: 0.09280733350266689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 79, Train Loss: 0.03791804730324913, Val Loss: 0.09318139773115641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80, Train Loss: 0.03847005410679672, Val Loss: 0.09338204968889283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 81, Train Loss: 0.03816146563955275, Val Loss: 0.09399395356088697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:01<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 82, Train Loss: 0.03878569026385626, Val Loss: 0.09230499797317407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 3/300 [00:03<04:19,  1.14it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7af4780be560>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 43, in wait\n",
            "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 27, in poll\n",
            "    pid, sts = os.waitpid(self.pid, flag)\n",
            "KeyboardInterrupt: \n",
            "  1%|          | 3/300 [00:03<06:03,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b5d7c27054fd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Validation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}